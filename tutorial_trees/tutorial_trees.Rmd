---
title: "Tutorial 2: Tree-based methods (DSE 2021)"
author: Madina Kurmangaliyeva
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
tutorial:
  id: "dse_mk.tutorial_trees.v1"
  version: 1
runtime: shiny_prerendered
description: "Decision trees and tree-based methods"
---

## Introduction

In this tutorial we will be working with a small sample from [COMPAS data](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis).  The dataset we are loading does not represent all the variables used by the original COMPAS software, but rather a stylized and cleaned version of it to learn in class. The dataset originally comes from package _fairness_. See the [documentation.](https://www.rdocumentation.org/packages/fairness/versions/1.0.1/topics/compas). I sampled 500 observations of only *Caucasian* or *African-American*  offenders.
Our goal will be to predict recidivism using classification trees from `tree` package.

```{r setup, echo=FALSE}
# This is the setup code. It loads necessary libraries and the data from a csv file. Uncomment the line below in case you are running this tutorial on your own computer and you need to install those packages first.
# install.packages(c("learnr", "tidyverse", "tree"))
library(learnr)
tutorial_options(exercise.timelimit=60*10)
library(tidyverse)
library(tree)
library(randomForest)
library(gbm)

dataset <- read.csv("./data/compas_sample500.csv", stringsAsFactors = TRUE)

set.seed(234)
train <- sample(x = c(0, 1), size = 500, replace = TRUE, prob = c(0.5, 0.5))

set.seed(345)
tree_compas <- tree(Two_yr_Recidivism ~ . ,
                    data = dataset[train == 1, ],
                    split = "gini",
                    minsize = 25,
                    model = TRUE)
set.seed(789)
cv_compas <- cv.tree(tree_compas, FUN = prune.misclass, K = 10)

best_n_leaves <- rev(cv_compas$size)[which.min(rev(cv_compas$dev))]

# Pruning the tree
prune_compas <- tree_compas %>% prune.misclass(., best = best_n_leaves)

# Create predictions for the test observations
set.seed(456)
tree_predict <- predict(object = prune_compas, newdata = dataset[train==0, ], type = "class")


test_y <- dataset$Two_yr_Recidivism[train==0]

set.seed(333)
bag_compas <- randomForest(Two_yr_Recidivism ~ ., 
                           data = dataset[train == 1, ], 
                           ntree = 1000,
                           mtry = ncol(dataset) - 1,
                           importance = TRUE)

set.seed(123)
bag_predict <- predict(object = bag_compas,
                        newdata = dataset[train==0, ],
                        type = "response")

optimal_mtry <- floor(sqrt(6))

set.seed(444)
rf_compas <- randomForest(Two_yr_Recidivism ~ ., 
                           data = dataset[train == 1, ], 
                           ntree = 1000,
                           mtry = optimal_mtry,
                           importance = TRUE)

rf_predict <- predict(object = rf_compas,
                      newdata = dataset[train==0, ],
                      type = "response")

dataset_edited <- dataset %>% 
  mutate(Two_yr_Recidivism = as.integer(Two_yr_Recidivism=="yes"))

set.seed(555)

boost_compas <- gbm(Two_yr_Recidivism ~ . , 
                    data = dataset_edited[train==1, ], 
                    shrinkage=0.01,
                    bag.fraction = 0.5,
                    distribution = "bernoulli", 
                    n.trees = 1000, 
                    interaction.depth = 2,
                    cv.folds = 2,
                    n.cores=1) 

best_ntrees_boost <- gbm.perf(boost_compas, method="cv", plot.it = FALSE)

set.seed(555)
boost_predict_num <- predict(object = boost_compas,
                         newdata = dataset[train==0, ],
                         n.trees = best_ntrees_boost) 

boost_predict <- ifelse(boost_predict_num > 0, "yes", "no") %>%
  as.factor()
```

```{r setup-public}
# This is the setup code. It loads necessary libraries and the data from a csv file. Uncomment the line below in case you are running this tutorial on your own computer and you need to install those packages first.
# install.packages(c("learnr", "tidyverse", "tree"))
library(learnr)
library(tidyverse)
library(tree)

dataset <- read.csv("./data/compas_sample500.csv", stringsAsFactors = TRUE)
```


You can inspect the dataset with any of the following commands:
  
  
```{r inspect, exercise = TRUE}
# dataset %>% names()
# dataset %>% head()
# dataset %>% glimpse()
dataset %>% str()
```


**COMPAS variables:**

- **Two_yr_Recidivism:** factor, yes/no for recidivism or no recidivism. This is the outcome or target in this dataset
- **Age_Above_FourtyFive:** factor, yes/no for age above 45 years or not
- **Age_Below_TwentyFive:** factor, yes/no for age below 25 years or not
- **Female:** factor, female/male for gender
- **Misdemeanor:** factor, yes/no for having recorded misdemeanor(s) or not
- **ethnicity:** factor, Caucasian, African American


### Exercise: Count missing observations

The code below counts the number of missing observations per each variable:

* `summarise_all` computes a summary statistic for each variable in the dataset. 
* To each variable, we apply the function `~sum(is.na(.))` to sum the total number of missing observations.

Change the code below to obtain the average of each  variable using function `mean()` instead.

```{r summarise, exercise = TRUE}
dataset %>% summarise_all(~sum(is.na(.))) 
```

Why do you get `NA` for each variable except `Number_of_Priors`?

None of the variables has any missing values. 

Try changing the formula in the code above to `mean(.)` instead of `sum(is.na(.))`. Why do you get `NA` for each variable except `Number_of_Priors`?


### Exercise: Number of priors by ethnicity and gender

How does number of priors vary by ethnicity? How many obs are there per ethnicity?

The code below gives answers to these questions. As you can see white Americans have lower than average prior convictions, while Black Americans have higher than average prior convictions. Also, roughly 3/5 of our sample are Black Americans.

```{r group_ethnicity, exercise = TRUE, exercise.eval=TRUE}
dataset %>% 
  group_by(ethnicity) %>% 
  summarise(mean_priors = mean(Number_of_Priors),
            n = n())
```

```{r group_ethnicity-hint}
dataset %>% 
  group_by(ethnicity, Female) %>% 
  summarise(mean_priors = mean(Number_of_Priors))
```

**Edit the code above** such that it shows the average number of priors by *ethnicity* AND *gender* and answer the following question: 

```{r quiz_gender, echo=FALSE}
quiz(
  question("Based on the information from the descriptive table:",
    answer("Men have on average fewer priors than women"),
    answer("White women have the lowest number of priors on average", correct = TRUE),
    answer("Black women have fewer priors than white men", correct = TRUE)))
```

### Exercise: Visualize the data

You can also visualize some variables using the following code

```{r histogram, exercise=TRUE}
dataset %>% 
  ggplot(aes(x = Number_of_Priors)) + 
  geom_histogram()
```

* We used ggplot package 
* In the aesthetics function `aes(x = ?, y = ?)` you specify which variables of the data to plot. 
* To vary the color depending on another variable `z`, add `aes(x, y, color = z)`. 
* To vary the shape of the point depending on another variable `z`, add `aes(x, y, shape = z)`. 
* Histograms do not require `y`, since `y` is the count automatically generated by ggplot.


**Amend the code below**, such that it draws two separate densities for men and women:

```{r density_plot, exercise=TRUE}
dataset %>% 
  ggplot(aes(x = Number_of_Priors)) + geom_density()
```

```{r density_plot-hint}
dataset %>% 
  ggplot(aes(x = Number_of_Priors, color = Female)) + geom_density()
```


**Compare** this graph to the previous graph you just made. What is different?

```{r faceted_plot, exercise = TRUE}
dataset %>% 
  ggplot(aes(x = Number_of_Priors)) + geom_density(color = "red") + 
  facet_grid(~Female)
```

```{r faceted_plot-hint}
dataset %>% 
  ggplot(aes(x = Number_of_Priors, color = ethnicity)) + geom_density() + 
  facet_grid(~Age_Below_TwentyFive)
```


Amend the code above to  make a density plot of the *number of priors* colored by `ethnicity` and faceted by `Age_Below_TwentyFive`. Do number of priors correlate with age?





## Classification tree


```{r quiz_classification, echo = FALSE}
quiz(
  question("Why is predicting recidivism a classification problem?",
    answer("Because the target variable is a categorical variable", correct = TRUE),
    answer("Because independent variables include categorical variables"),
    answer("Because the target variable is a continuous variable"),
    answer("Because independent variables include continuous variables")
  )
)
```



### Split data into training and test sets

This time, let's use another approach than last week to assign observations to training and test sets.
Let's generate an indicator vector for the observations that need to be included in the training set. The vector should have length 500 (same as number of observations) and consist of only zeros (do not include in the training set) and ones (include in the training set).  

**Modify the code below** to get the vector of size 500 (as many as there are observations) where approximately half of it is zeros and half is ones. We create this vector by randomly sampling 500 times from a vector `c(0,1)` with equal probability. The final two lines check how many zeros and how many ones are in the just created vector `train`.

```{r generate_train_index, exercise=TRUE, exercise.eval=FALSE}
# Example: sample(x = c(a,b), size = 100, replace =  TRUE, prob = c(0.3, 0.7))

set.seed(234)
train <- sample(x = c(_, _), size = ____, replace = TRUE, prob = c(__, __))

sum(train == 0)
sum(train == 1)
sum(train == 1) + sum(train == 0)
```

```{r generate_train_index-hint}
set.seed(234)
train <- sample(x = c(0, 1), size = 500, replace = TRUE, prob = c(0.5, 0.5))

sum(train==0)
sum(train==1)
sum(train==1) + sum(train==0)
```

```{r quiz_trainingsize, echo=FALSE}
quiz(
  question("How many observations are assigned to the training set?",
    answer("247"),
    answer("253", correct = TRUE),
    answer("500")
  )
)
```




###  Train the classification tree

Now, we fit a classification tree using  the training sample and `tree()` function from `tree`   package.

**Good news #1:** We can work directly with dataframes, no need to transform it into a matrix first to feed into the `tree()` function.

**Good news #2:**  We can directly work with factor variables without transforming them into dummies. 


We train the classification tree using *Gini index* as objective criteria: `split = "gini"`. (We could  have  used `split = "deviance"` option instead to use misclassification error as the objective criteria.) We are also putting `minsize = 25`: the minimum leaf size requirement of 20 observations, i.e., every terminal node should contain at least 25 observations).

**Finish the code below** by putting the correct formula and don't forget to use only the training set. At the end, ask the summary of the classification tree you have just trained.


```{r tree_train, exercise = TRUE, exercise.eval=FALSE}
set.seed(345)
tree_compas <- tree::tree(__________ ~ . ,
                    data = _________, 
                    split = ______,
                    minsize = __,
                    model = TRUE)
summary(___)
```

```{r tree_train-hint}
set.seed(345)
tree_compas <- tree(Two_yr_Recidivism ~ . ,
                    data = dataset[train == 1, ],
                    split = "gini",
                    minsize = 25,
                    model = TRUE)
summary(tree_compas)
```


```{r quiz_howmanyleaves, echo = FALSE}
quiz(
  question("How many leaves does this tree have?",
    answer("253"),
    answer("19", correct = TRUE),
    answer("65")
  )
)
```




### Exercise:  Visualize the tree

**Modify the code below** to get the plot of the tree. `plot()` function plots the tree, while `text()` function adds annotations to each node.*


```{r plot_tree, exercise=TRUE, exercise.eval=FALSE, fig.height = 12, fig.width=8}
plot(____)
text(____, pretty = 0)
```

```{r plot_tree-hint}
plot(tree_compas)
text(tree_compas, pretty = 0)
```

```{r quiz_whichvars, echo=FALSE}
quiz(
  question("Which variables does the tree use to predict  recidivism? [multiple answers possible]",
    answer("number of priors", correct = TRUE),
    answer("ethnicity", correct = TRUE),
    answer("age above 45", correct = TRUE),
    answer("misdemeanor")
  )
)
```

Remember that the very first split  of a decision tree usually indicates the most important predictor for Y. In the end, the very first split finds the variable that helps the most in minimizing RSS.

```{r quiz_whichvarimportant, echo=FALSE}
quiz(
  question("Which variable is the most important in predicting recidivism?",
    answer("number of priors", correct = TRUE),
    answer("ethnicity"),
    answer("age above 45"),
    answer("misdemeanor")
  )
)
```

```{r quiz_predict, echo=FALSE}
quiz(
  question("A female offender with the number of priors equal 0.2 and aged above 45 is predicted to",
    answer("Recidivate", correct = TRUE),
    answer("Not recidivate" ),
    answer("No prediction available")
  )
)
```


### Tree pruning

Now let's apply some pruning to our big tree.


**Fix the code below** by feeding `tree_compas` to the `cv.tree()` function for a 10-fold cross-validation

```{r pruning, exercise=TRUE, exercise.eval=FALSE}
set.seed(789)
cv_compas <- cv.tree(____, FUN = prune.misclass, K = __)
cv_compas
```

```{r pruning-hint}
set.seed(789)
cv_compas <- cv.tree(tree_compas, FUN = prune.misclass, K = 10)
cv_compas
```



Let's plot the tree size and the number of misclassified cases. As you can see from the plot, a much shallower tree could achieve a lower misclassification error.

```{r graph_pruning_size, exercise = TRUE}
tibble(size = cv_compas$size, n_misclassified = cv_compas$dev) %>%
  ggplot(aes(x = size, y = n_misclassified)) +
  geom_point() +
  geom_line(linetype="dashed") +
  theme_bw()

```

Hence, let's actually prune our main tree using `prune.misclass()` function on `tree_compas`, where

```{r pruning_main_tree, exercise = TRUE}
best_n_leaves <- rev(cv_compas$size)[which.min(rev(cv_compas$dev))]

cat("The lowest classification error is achieved already at ", best_n_leaves, " leaves\n")

# Pruning the tree
prune_compas <- tree_compas %>% prune.misclass(., best = best_n_leaves)
```

and plot it

*Write the code to plot the pruned tree. Hint: use `plot()` and `text()` functions*

```{r plot_pruned_tree, exercise=TRUE, exercise.eval=FALSE}

```

```{r plot_pruned_tree-hint}
plot(prune_compas)
text(prune_compas, pretty = 0)
```

Ah! Much better!

```{r quiz_pruned, echo=FALSE}
quiz(
  question("Compared to the unpruned tree, the pruned tree:",
           answer("lost most of its left branches", correct = TRUE),
           answer("lost most of its right branches"),
           answer("is now shallower", correct = TRUE)
  )
)
```

### Test sample predictions

Let's  generate predictions based on the pruned tree for the **test sample** using `predict()` function and store them as a separate vector. You should choose `type = "class"` because you are interested in the prediction of the class (not in the vector of probabilities over different classes).

**Fix the code below** to store the predictions for the test sample and then print the first ten predictions:

```{r pruned_error, exercise=TRUE, exercise.eval=FALSE}
# Create predictions for the test observations
set.seed(456)
tree_predict <- predict(object = ___tree_object_____, newdata = ______, type = "class")


# Show the first ten predictions
________[__:__]
```

```{r pruned_error-hint}
# Create predictions for the test observations
set.seed(456)
tree_predict <- predict(object = prune_compas, newdata = dataset[train==0, ], type = "class")


# Show the first ten predictions
prunedtree_predict[1:10]
```




### Misclassification errors and fairness


Let's properly generate the confusion matrix for the test data (i.e., a matrix that counts number of correctly and incorrectly predicted observations).

The code counts the  number of observations within the interacted values of `Two_yr_Recidivism` (the truth) and `tree_predict` (the prediction). Then, it spreads the result using the values of `tree_predict` as column names and `n` as values that populate the columns.

```{r confusion_matrix, exercise=TRUE}
# For convenience, store the outcomes of the test sample in a separate vector
test_y <- dataset$Two_yr_Recidivism[train==0]

# Create confusion matrix
data.frame(Two_yr_Recidivism = test_y,
          tree_predict = tree_predict) %>% 
  count(Two_yr_Recidivism, tree_predict) %>%
  spread(tree_predict, n, sep = "_")
```

```{r quiz_nrecidmisclass, echo=FALSE}
quiz(
  question("How many offenders were predicted to not recidivate while in fact they did recidivate?",
    answer("73"),
    answer("55"),
    answer("26", correct = TRUE),
    answer("93")
  )
)
```

### Exercise -- calculate misclassification error


We can also calculate the misclassification error (i.e., share of observations in the test data which have been wrongly classified )

*Modify the code below to get the misclassification error of the tree you trained*

```{r misclass_error, exercise=TRUE, exercise.eval=FALSE}
data.frame(Two_yr_Recidivism = test_y,
          tree_predict = tree_predict
          ) %>% 
  summarise(mean(_______ != ________))
```

```{r misclass_error-hint}
data.frame(Two_yr_Recidivism = test_y,
          tree_predict = tree_predict
          ) %>% 
  summarise(mean(Two_yr_Recidivism != tree_predict))
```

```{r quiz_misclassificationerror, echo=FALSE}
quiz(
  question("What does 0.328 misclassification error mean? It means that using this classification tree to predict future recidivism:",
    answer("we expect to misclassify 32.8% of offenders", correct = TRUE),
    answer("we expect to misclassify 32.8% of offenders who will recidivate"),
    answer("we expect to misclassify 32.8% of offenders who will not recidivate"),
    answer("we expect to classify only 35.2% of offenders correctly")
  )
)
```

### Exercise: calculate misclassification error by ethnicity

Recalculate the misclassification error for each ethnicity group separately.

**Enter your code below** to get the misclassification error by ethnicity. Hint: Use `group_by()` function as we did in previous tutorials. Hint: You need to `group_by()` ethnicity

```{r misclass_error_ethnicity, exercise=TRUE, exercise.eval=FALSE}

```

```{r misclass_error_ethnicity-hint}
data.frame(Two_yr_Recidivism = test_y,
          tree_predict = tree_predict,
          ethnicity = dataset$ethnicity[train==0]
          ) %>% 
  group_by(ethnicity) %>%
  summarise(mean(Two_yr_Recidivism != tree_predict))

```

As you can see, African Americans are less likely to be misclassified than Caucasians.

### Exercise: calculate False-positive error rate by ethnicity

Calculate how many  offenders who **do not** recidivate have been erroneously predicted to recidivate?


*Enter your code below to get the false positive error by ethnicity. Hint: You need to `filter()` data to keep only the offenders who will not recidivate*

```{r fp_ethnicity, exercise=TRUE, exercise.eval=FALSE}

```



```{r fp_ethnicity-hint}
data.frame(Two_yr_Recidivism = test_y,
          tree_predict = tree_predict,
          ethnicity = dataset$ethnicity[train==0]
          ) %>% 
  filter(Two_yr_Recidivism=="no") %>% 
  group_by(ethnicity) %>%
  summarise(mean(Two_yr_Recidivism != tree_predict))
```

```{r quiz_fp, echo=FALSE}
quiz(
  question("What do you find after finding the false positive rates across the two ethnic groups?",
    answer("The mistakes are equally likely across ethnic groups"),
    answer("The classification tree is more likely to misclassify the non-recidivating Black offenders, than white offenders", correct = TRUE),
    answer("Among the non-recidivating African Americans 48% were falsely predicted to recidivate", correct = TRUE),
    answer("Among therecidivating African Americans 48% were not predicted to recidivate"),
    answer("48% of African Americans recidivate")
  )
)
```

### Exercise: calculate False-Negative error rate by ethnicity

Calculate how many  offender who **will** recidivate are being erroneously predicted not to recidivate?


**Enter your code below** to get the false negative error by ethnicity. Hint: You need to `filter()` data to keep only the offenders who will recidivate

```{r fn_ethnicity, exercise=TRUE, exercise.eval=FALSE}

```



```{r fn_ethnicity-hint}
data.frame(Two_yr_Recidivism = test_y,
          tree_predict = tree_predict,
          ethnicity = dataset$ethnicity[train==0]
          ) %>% 
  filter(Two_yr_Recidivism=="yes") %>% 
  group_by(ethnicity) %>%
  summarise(mean(Two_yr_Recidivism != tree_predict))
```

```{r quiz_fn, echo = FALSE}
quiz(
  question("What do you find after finding the false negative rates across the two ethnic groups?",
    answer("The mistakes are equally likely across ethnic groups"),
    answer("The classification tree is more likely to misclassify the recidivating Black offenders, than white offenders"),
    answer("Among the recidivating African Americans 16.7% were mistakenly labeled as non-recidivating", correct = TRUE),
    answer("16.7% of African Americans recidivate")
  )
)
```

### Exercise: calculate Positive Predictive Value across ethnicities

In medical diagnostic tests, positive predictive value (PPV) is "the probability that subjects with a positive screening test truly have the disease". Positive predictive value is the share of true positive cases out of all cases which have been flagged as positive:

$$PPV = \frac{TP}{TP+FP}$$
**Enter your code below** to calculate the PPV by ethnicity. 

```{r ppv_ethnicity, exercise=TRUE, exercise.eval=FALSE}

```



```{r ppv_ethnicity-hint}
data.frame(Two_yr_Recidivism = test_y,
          tree_predict = tree_predict,
          ethnicity = dataset$ethnicity[train==0]
          ) %>% 
  filter(tree_predict=="yes") %>% 
  group_by(ethnicity) %>%
  summarise(mean(Two_yr_Recidivism == "yes"))
```

```{r quiz_ppv, echo = FALSE}
quiz(
  question("What do you find after finding the positive predictive values across the two ethnic groups?",
    answer("The PPV is the same across ethnic groups"),
    answer("Among those predicted to recidivate, the decision tree makes more errors for African Americans, rather than for white Americans"),
    answer("Among those predicted to recidivate, the decision tree makes more errors for white Americans, rather than for African Americans", correct = TRUE)
  )
)
```

## Bagging and Random Forest

Do you remember what is the difference between bagging trees and random forest (from the video lecture)?

```{r quiz_bagvsrf, echo=FALSE}
quiz(
  question("What is the difference between bagging trees and Random Forest (RF)?",
    answer("Bagging grows many decision trees, while RF grows just one big tree testing only a random subset of predictors at each potential split"),
    answer("Both RF and Bagging grow many decision trees, but RF also prunes each one before aggregating the decisions"),
    answer("Both RF and Bagging fit many decision trees and summarize the information from the trees in the same way, but bagging evaluates all predictors for a split, while RF tests only a random subset of predictors at each potential split", correct = TRUE)
  )
)
```


### Bagging trees

We keep predicting `Two_Yr_Recidivism` using all other variables in the dataset as predictors. Remember, that bagging is a special case of Random Forest, which uses all predictors to find an optimal potential split. Hence, we can simply use `randomForest()` function from the `randomForest` package to implement *bagging* by specifying the parameter `mtry` -- the number of variables randomly samples as candidates -- to be equal to the number of all predictors.  In our dataset there are six predictors. Also,  ask to grow 1000 trees.


```{r ask-rf, echo = TRUE}
library(randomForest)
```


**Fill in the blanks below** to implement bagging. Hint: `ncol(dataset) - 1` gives the number of columns in the dataset minus one -- the target variable). 

```{r bagging, exercise=TRUE, exercise.eval=FALSE,  exercise.timelimit = 360}
set.seed(333)
bag_compas <- randomForest(____ ~ ., 
                           data = dataset[train == _, ], 
                           ntree = ____,
                           mtry = ___,
                           importance = TRUE)

summary(bag_compas) # Shows you the objects that are included in the object
```

```{r bagging-hint}
set.seed(333)
bag_compas <- randomForest(Two_yr_Recidivism ~ ., 
                           data = dataset[train == 1, ], 
                           ntree = 1000,
                           mtry = ncol(dataset) - 1,
                           importance = TRUE)

summary(bag_compas) # Shows you the objects that are included in the object
```



### Exercise -- Importance of predictors

Remember that in a decision tree the very first split usually uses the most important predictor. Since we now aggregate many decision trees, we can no longer represent our predictions graphically (in a form of a tree). However, we still can access the calculation of the relative predictive value of different predictors using the function `importance()` to which we feed our fitted bagged trees.

**Fill in the blank below** to get the relative importance of predictors in the bagging model

```{r bagimp, exercise=TRUE, exercise.eval=FALSE}
importance(_____)
```

```{r bagimp-hint}
importance(bag_compas)
```

The first measure "MeanDecreaseAccuracy" is computed using out-of-bag data (see a dedicated subsection in Chapter 8 of ISLR about this method). Remember, that bagging uses a random draw of observations each time to fit a new tree. In short, out-of-bag method is another clever way to approximate test errors by using the sample of data that have not been used by the bagging procedure to grow a given current tree.  In other words, "for each tree, the prediction error on the out-of-bag portion of the data is recorded (error rate for classification, MSE for regression). Then the same is done after permuting each predictor variable. The difference between the two are then averaged over all trees, and normalized by the standard deviation of the differences." Hence, a predictor which permutation leads to the greatest decrease on the mean accuracy is considered to be the most important.

The second measure "MeanDecreaseGini" is the total decrease in node impurities from splitting on the variable, averaged over all trees. ("For classification, the node impurity is measured by the Gini index. For regression, it is measured by residual sum of squares.") 

```{r quiz_bagimportance, echo=FALSE}
quiz(
  question("According to the results of the bagging procedure on COMPAS dataset, we learn that:",
    answer("According to both measures -- out-of-bag accuracy and node purity -- number of priors is the most important predictor of recidivism", correct = TRUE),
    answer("Age above 45 is the second most important predictor according to the out-of-bag accuracy measure"),
    answer("Age below 25 is the second most important predictor according to the out-of-bag accuracy measure", correct = TRUE),
    answer("Ethnicity is the second most important predictor according to node purity measure", correct = TRUE)
  )
)
```


Let's save the bagging model predictions:

```{r bag-predict, exercise=TRUE}
set.seed(123)
bag_predict <- predict(object = bag_compas,
                        newdata = dataset[train==0, ],
                        type = "response")
```



### Random Forest

```{r quiz_rf, echo=FALSE}
quiz(
  question("Random Forest randomly subsets m predictors to try at each split. What is a typical choice of m?",
    answer("m = 3"),
    answer("m ≈ the floor of sqrt(p)", correct = TRUE),
    answer("m ≈ p/2")
  )
)
```

Now you are ready to grow a Random Forest. Please use a typical choice of m and `ntree= 1000`. Save the predictions.

```{r rf, exercise=TRUE, exercise.eval=FALSE,  exercise.timelimit = 360,  exercise.lines = 15}
optimal_mtry <- floor(sqrt(6))
cat("The optimal choice of m is ", optimal_mtry, "\n")

set.seed(444)
rf_compas <- randomForest(#put code)

rf_predict <- predict(#put code)
```

```{r rf-hint}
optimal_mtry <- floor(sqrt(6))
cat("The optimal choice of m is ", optimal_mtry, "\n")

set.seed(444)
rf_compas <- randomForest(Two_yr_Recidivism ~ ., 
                           data = dataset[train == 1, ], 
                           ntree = 1000,
                           mtry = optimal_mtry,
                           importance = TRUE)

rf_predict <- predict(object = rf_compas,
                      newdata = dataset[train==0, ],
                      type = "response")
```

Check importance of variables for the random forest `rf_compas`. Has anything changed in comparison to the bagging procedure results?

```{r rfimp, exercise=TRUE, exercise.eval=FALSE}

```

```{r rfimp-hint}
importance(rf_compas)
```

## Boosting

```{r quiz_boosting, echo=FALSE}
quiz(
  question("What is the difference between bagging and boosting?",
    answer("Bagging grows many decision trees and then averages their prediction, while boosting grows trees sequentially, where each new tree tries to 'attack' the  residuals unexplained by previous trees", correct = TRUE),
    answer("Boosting procedure can be applied only to decision trees, while bagging can be used for any other models (e.g., OLS)"),
    answer("Bagging improves with the number of trees it grows, while boosting may worsen due to overfitting", correct = TRUE)
  )
)
```


To implement tree boosting we will use function `gbm()` from the package `gbm` (generalized boosted regression models). 

```{r setup-boosting, echo=TRUE}
library(gbm)
```


You need to remember, that this package does not accept target variables in a factor form, but only as `1/0` vectors. In other words we need to convert the "yes" and "no" responses of `Two_yr_Recidivism`  into  1 and 0 numbers.

To do so, we create a separate dataframe called `dataset_edited` in which we mutate our target variable to a conformable form (Note that copying the entire dataset is not the most efficient way of doing it. However, since we are dealing with a very small dataset and want to avoid confusion, we create a separate copy.)


```{r editeddata, echo = TRUE}

dataset_edited <- dataset %>% 
  mutate(Two_yr_Recidivism = as.integer(Two_yr_Recidivism=="yes"))

```


###  Boosting

Boosting comes with much more parameters to choose than any previous tree-based models:

* `shrinkage` -- the shrinkage parameter which controls the speed of learning from new trees -- usually set somewhere between 0.001 to 0.1
* `bag.fraction` -- the fraction of observations randomly selected for training the next tree
* `distribution` -- specifies the distribution of target variable (e.g., "gaussian" and "bernoulli", but also "laplace", "poisson", etc. ); note that in `randomForest` we did not need to specify it, since it automatically recognized that our target variable was a factor.
* `interaction.depth` -- specifies the maximum depth of tree. Remember that in boosting we can even grow trees with depth 1, i.e. assigning a very small task to each "ant", but the true interaction depth comes from the combination of many "ants"  working sequentially like in a relay race.
* `cv.folds` -- specifies number of cross-validation folds to perform. Remember, that unlike in random forest, higher number of trees in boosting may  lead to **overfitting**. Hence, we need to search for the optimal number of trees to grow through cross-validation.



You can see the relative importance of predictors using `summary` function.

*Fill in the blanks in the code below*  to train boosted trees to predict `Two_yr_Recidivism` using the `dataset_edited`. Shrink at 0.01, allow to select only a half of the sample randomly to grow each new tree, where each tree has at max depth two. Grow 1000 trees but ask to do 2-fold cross validation.  Since our target  variable takes only values of 0 and 1, use Bernoulli distribution.


```{r boost, exercise=TRUE, exercise.eval=FALSE, exercise.timelimit = 360}
set.seed(555)

boost_compas <- gbm(_______ ~ . , 
                    data = dataset_edited[train==1, ], 
                    shrinkage=____,
                    bag.fraction = ____,
                    distribution = "_______", 
                    n.trees = _____, 
                    interaction.depth = _,
                    cv.folds = _,
                    n.cores=1) # Not setting n.cores  parameter leads to mistakes
summary(_____) # Gives variable importance
```


```{r boost-hint}
set.seed(555)

boost_compas <- gbm(Two_yr_Recidivism ~ . , 
                    data = dataset_edited[train==1, ], 
                    shrinkage=0.01,
                    bag.fraction = 0.5,
                    distribution = "bernoulli", 
                    n.trees = 1000, 
                    interaction.depth = 2,
                    cv.folds = 2,
                    n.cores=1) # Not setting n.cores parameter leads to mistakes
summary(boost_compas) # Gives variable importance
```





### Find optimal number of trees for boosting

The next step of boosting procedure is to retrieve the number of trees that minimize the expected errors. You can access it using `gbm.perf()` function, and it even gives you a nice plot of training and validation errors.

*Fill in the blanks in the code below
```{r boost_best, exercise=TRUE, exercise.eval=FALSE}
best_ntrees_boost <- ___.____(boost_compas, method="cv", plot.it = TRUE)
```

```{r boost_best-hint}
best_ntrees_boost <- gbm.perf(boost_compas, method="cv", plot.it = TRUE)
```



In the graph, black line plots the (in-sample) training errors as a function of number of trees, while the green line plots the (out-of-sample) validation errors. 

```{r quiz_cvboost, echo=FALSE}
quiz(
  question("From the plot you can see that:",
    answer("The in-sample fit always improves with the number of trees", correct = TRUE),
    answer("The in-sample fit is not affected by the number of trees"),
    answer("The out-of-sample fit always improves with the number of trees"),
    answer("The out-of-sample fit first deteriorates and then improves with the number of trees because of overfitting"),
    answer("The out-of-sample fit first improves and then deteriorates with the number of trees because of overfitting", correct = TRUE)
  )
)
```

In general, we see the manifistation of the Bias-Variance trade-off again and again, in every single prediction task.

Let's run the boosting model with the optimal number of trees, and make predictions.

```{r best-boosting, exercise=TRUE}
set.seed(555)

boost_predict_num <- predict(object = boost_compas,
                         newdata = dataset[train==0, ],
                         n.trees = best_ntrees_boost) 
```



## Accuracy of different models 

Finally, it is time to put all of the tree-based models in one row and compare their performance.

We generate predictions for every method -- `tree_predict` for pruned tree, `bag_predict` for  bagging, `rf_predict` for random forest, `boost_predict` for boosting -- using the test data. 

Note that the predict function for boosting returned back numerical values. We have to convert those predictions again back to a categorical variables by using a logical evaluation whether the predicted value is higher than zero. If it is, then the boosting algorithm predicts that the offender will recidivate. Then, we convert the logical vector into an initial coding of "yes" and "no" values of `Two_yr_Recidivism`.


```{r boost-predict-convert, exercise = TRUE}

# Convert predictions by boosting from a logical vector into a factor of "yes/no" 
boost_predict <- ifelse(boost_predict_num > 0, "yes", "no") %>%
  as.factor()

str(boost_predict)

```


### Exercise -- compute the accuracy rate for different models

We want to compare the classification errors of our models in the test data. The code below is one way to do it. We creating a dataframe where we store the logical values of whether the model's predictions were CORRECT. Finally, we summarize the accuracy for each model by asking to give the mean of each column.

*Complete the code below*

```{r compare, exercise=TRUE, exercise.eval=FALSE}

data.frame(
    acc_prunedtree = ______ == test_y,
    acc_bag        = ______ == test_y,
    acc_rf         = ______ == test_y,
    acc_boost      = ______ == test_y
  ) %>%
  summarise_all(___)

```

```{r compare-hint}

data.frame(
    acc_prunedtree = tree_predict == test_y,
    acc_bag        = bag_predict  == test_y,
    acc_rf         = rf_predict   == test_y,
    acc_boost      = boost_predict== test_y
  ) %>%
  summarise_all(mean)

```

Note that you can always modify the code above to see how the classification rates differ by groups of offenders (using `group_by()` functionality of `dplyr` package).

```{r quiz_compare, echo=FALSE}
quiz(
  question("According to the table:",
    answer("The worst performing model is boosting"),
    answer("The worst performing model is bagging", correct = TRUE),
    answer("The best performing model  is boosting", correct = TRUE),
    answer("The best performing model  is random forest")
  )
)
```


